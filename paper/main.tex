\documentclass{elsarticle}

\usepackage[utf8x]{inputenc}
\usepackage[table]{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{graphicx}
\usepackage[table]{xcolor}
\usepackage{tabularx}
\usepackage{multirow,makecell}
\usepackage{float,lscape}
\usepackage[ruled,linesnumbered,vlined]{algorithm2e}
\usepackage{tkz-graph}
\usepackage{lscape} 
\usepackage[binary-units=true]{siunitx}

\definecolor{gold}{HTML}{FFF3D6}
\definecolor{silver}{HTML}{F3F5F7}
\definecolor{cupper}{HTML}{EAD9D7}
\fboxsep0pt
\newcommand{\firsto}{\colorbox{gold}{$m^1$}}
\newcommand{\firsts}{\colorbox{gold}{$s^1$}}
\newcommand{\firstu}{\colorbox{gold}{$u^1$}}
\newcommand{\firstp}{\colorbox{gold}{$p^1$}}
\newcommand{\secondo}{\colorbox{silver}{$m^2$}}
\newcommand{\seconds}{\colorbox{silver}{$s^2$}}
\newcommand{\secondu}{\colorbox{silver}{$u^2$}}
\newcommand{\secondp}{\colorbox{silver}{$p^2$}}
\newcommand{\thirdo}{\colorbox{cupper}{$m^3$}}
\newcommand{\thirds}{\colorbox{cupper}{$s^3$}}
\newcommand{\thirdu}{\colorbox{cupper}{$u^3$}}
\newcommand{\thirdp}{\colorbox{cupper}{$p^3$}}


\title{SAT Competition 2020\tnoteref{title}}
\tnotetext[title]{\url{satcompetition.github.io/2020}}

\author[jku]{Nils Froleyks}
\ead{nils.froleyks@jku.at}
\author[cmu]{Marijn Heule}
\ead{marijn@cmu.edu}
\author[kit]{Markus Iser}
\ead{markus.iser@kit.edu}
\author[hiit]{Matti JÃ¤rvisalo}
\ead{matti.jarvisalo@helsinki.fi}
\author[ctu]{Martin Suda} 
\ead{martin.suda@cvut.cz}

\address[kit] {
KIT Department of Informatics\\
\url{markus.iser@kit.edu}\\[1em]
}

\address[ctu] {
Czech Technical University in Prague, Czech Republic\\
\url{martin.suda@cvut.cz}\\[1em]
}


\newcommand{\todo}[1]{{\color{purple}Todo: #1}}

% Stack a variable number of arguments:
\makeatletter
\newcommand{\stack}[1]{%
\begin{tabular}{@{}l@{}}#1\checknextarg}
\newcommand{\checknextarg}{\@ifnextchar\bgroup{\gobblenextarg}{\end{tabular}}}
\newcommand{\gobblenextarg}[1]{\\#1\@ifnextchar\bgroup{\gobblenextarg}{\end{tabular}}}
\makeatother

\begin{document}

\begin{abstract}
SAT Competition 2020 stands in the tradition of the series of annual competitive events which motivate and assess the progress in SAT solving. 
This competition was special as it introduced the new cloud track where SAT Solvers that run on hundreds of processors could compete. 
Another novelty was the application-specific sub-track of the main track, 
where solvers competed in solving benchmark instances from one specific domain only, and in this year that was the planning domain. 
We used new tools to select and distribute benchmark instances and their attributes. 
In this paper we provide a description of the well-known and the new competition tracks and how we organized them. 
It follows then a detailed analysis of the results and the strategies of the award winning solvers. 
\end{abstract}

\begin{keyword}
SAT, Competition
\end{keyword}

\maketitle

\section{Introduction}

Propositional satisfiability (SAT) is the archetypal NP-complete problem. 
Despite its complexity, there is ongoing progress in solving methods and their implementations in SAT solvers. 
The generic nature of the SAT problem as well as the performance of state-of-the-art SAT solvers facilitate their use as the algorithmic backend in a plethora of applications. 
Today, SAT solvers are used for verification of hard- and software, product configuration, cryptography, planning and scheduling, to name a few. 
In mathematics, SAT solvers were recently even used to generate a record sized proof (200 TB) for an until then unsolved problem in Ramsey theory. 

SAT Competitions are organized regularly since 1992 in order to drive progress and to serve as a public assessment of the state-of-the-art in SAT solving. 
For each SAT Competition, a set of new benchmark instances is compiled which reflects the various applications of interest to the community. 

In this competition, we initiated a new application-specific subtrack. 
The idea is to provide a large amount of benchmark instances of a specific application, and to evaluate all solvers with respect to these instance in a special track. 
In this competition, we used 200 instances of the planning domain to run a planning subtrack. 

Faster networks, distributed systems, and the increasing number of processors in modern computers show that the trend of increasing parallelism is about to continue.  
In the new cloud track, for the first time, solver authors could compete in SAT technologies that allow hundreds of SAT solvers to cooperate in solving a SAT instance. 

\todo{Present some important pointers for SAT and CDCL}

In Section~\ref{sec:overview}, we start with an overview of the competition,
including detailed description of the competition tracks and their purpose,
the general rules and the used computing environment.
Section~\ref{sec:instances} contains an overview on the selected benchmark instances and details about the selection process. 
In Section~\ref{sec:results}, we present the competition results alongside a survey on the winning strategies of the award winning solvers. 
A meta-analysis of these results follows in Section~\ref{sec:analysis}, and we conclude with Section~\ref{sec:conclusion}.

\todo{maybe mention \url{https://satcompetition.github.io/2020/}}

\todo{Maybe mention proceedings \cite{SC2020}}

\section{Overview of the Competition}
\label{sec:overview}

TODO: Fugenelement

\subsection{Competition Tracks}

SAT Competition 2020 consisted of four tracks:
the Main track, Incremental Library track, Parallel track,
and the Cloud Track for massively parallel SAT solvers using up to 1024 cores. 
Furthermore, the Main track comprised of several sub-tracks.

\subsubsection{Main Track}

The focus of the traditional Main track is on sequential SAT solvers and their evaluation on structured, non-random benchmarks coming from various application areas. To participate in the main track, solvers needed to output certificates both for the satisfiable and the unsatisfiable answer. Moreover, the source code of the solver had to be made publicly available. 

Solvers not complying with either of the above criteria were only evaluated in a so-called No-Limits sub-track and were not eligible for the Main track awards. The No-Limits thus enabled participation of closed-source solvers (not being able or willing to expose the source code for legal or other reasons) as well as portfolio solvers (combining two or more core SAT solvers developed by different groups of authors; c.f.\ Rules below). However, solvers in No-Limits still competed against all other solvers in the Main Track (hence, we consider it a sub-track). The No-Limits sub-track was only evaluated with respect to the benchmarks instances which were \emph{newly} submitted to SAT Competition 2020.

Part of the Main track was the Planning sub-track, in which we evaluated the solvers on 200 benchmarks 
which all came from the same application domain - automated planning. 
Solver submitted to the Main track automatically participated in the Planning sub-track.
It is envisioned, that also in the future there will be an application sub-track of the Main track, each time highlighting a different area where the SAT solving technology helps to advance the state of the art.

Finally, the Main Track also had the so-called ``Glucose hack'' sub-track. 
Since in the past several advances in SAT solving required only a small modification of an established solver
to achieve a considerable contribution, this sub-track encouraged participation 
of small modifications of the Glucose 3.0 SAT solver. The limit for being considered a ``hack''
was set to 1000 non-space character edit distance from the sources of Glucose provided by the organisers. 
Unfortunately, in 2020 there were not enough participants in this sub-track and we do not report on it 
in the results section.

\subsubsection{Incremental Library Track}

\subsubsection{Parallel Track}

\subsubsection{Cloud Track}

\subsection{Mandatory participation requirements}



\todo{Should we have a discussion (maybe later sections)
about the future of the portfolio solver rule
concerning the point raised by Mate Soos (in emails)?}

\subsection{Certificates}

In all tracks it was required to output a model to certify recognising a satisfiable instance.
On the other hand, certificates for unsatisfiable instances (proofs) were required only 
in the Main Track (besides the No Limits subtrack).

Proofs of unsatisfiability were to be emitted in the DRAT format~\cite{DRATtrim},
either in its textual version, which is very similar to the DIMACS input format,
or on a more compact binary version (for more details, see \cite{satComp2020www}, Unsat Certificates).

The proofs were validated in a two step fashion. First, the tool {\tt DRAT-trim}~\cite{DRATtrim}
was used for initial checking and optimizing the emitted proof, deriving a so-called LRAT proof file.
Afterwards, an independent, formally-verified checker {\tt cake\_lpr} \cite{cakeLprGithub}, 
was used for validating the LRAT proof as a correct proof of unsatisfiability of the given formula.
We only considered solved those unsatisfiable formulas that could be validated by {\tt DRAT-trim}.
There were several cases where {\tt cake\_lpr} ran out of resources before being able to 
confirm after {\tt DRAT-trim}. However, there was no case where {\tt DRAT-trim} would accept
a proof and {\tt cake\_lpr} would later disagree with the verdict.

\subsection{Computing environments}

\label{sec:computing}
The Main Track with its subtracks was run on the StarExec cluster \cite{starexec},
whose nodes are equipped with Intel Xeon \SI{2.4}{\giga\hertz} processors 
and \SI{128}{\giga\byte} of memory.
The time limit enforced on each solver for solving an instance was \SI{5000}{\second}. 
The solvers were allowed to use up to the full \SI{128}{\giga\byte} of RAM.\footnote{
Unfortunately, the memory limit of \SI{24}{\giga\byte}, that was used in the previous years,
was by mistake advertised on the competition web page prior to solver submission.
This could have resulted in some solvers not ``daring'' to use the full \SI{128}{\giga\byte}
in the competition.}

The Incremental Library Track was run on computers with 2x Intel Xeon E5430 \SI{2.66}{\giga\hertz}
(4-Core) processors and \SI{24}{\giga\byte} of RAM.

\todo{Parallel / Cloud}

\section{Description of Benchmark Instances}
\label{sec:instances}

For selection of benchmark instances in this competition, we used GBD Tools\footnote{\url{https://pypi.org/project/gbd-tools/}}. 
GBD Tools is a tool-chain for maintenance and distribuation of benchmark instances and instances features~\cite{Iser:2018:GBD}. 
Its database\footnote{\url{https://gbd.iti.kit.edu}} contains information about all instances used in SAT competitive events dating back to 2006. 
In GBD Server, each instance also has an URL, such that instances can also be distributed by using their identifier: \url{https://gbd.iti.kit.edu/file/<gbd-hash>}.

Using the concept of instance identification via GBD Hash, which is the hash-sum of the unpacked and normalized DIMACS file, GBD Tools provide utilities to query for instances with specific properties, while maintaining the association of instance-id and instance attributes in publicly available databases.
Like this it is possible to query for instances with desired properties, e.g., instance author, family and result, which gives the instance selection process a new quality. 


\subsection{Selection of Instances}

The ``Bring Your Own Benchmarks'' (BYOB) rule (applied as of SAT Competition 2017~\cite{SC2017}) requires solver authors to submit $20$ new benchmark instances in order to participate in the competition. At least $10$ of these instances are required to be ``interesting'', whereby interesting is defined as Minisat~\cite{Niklas:2003:Minisat} needs at least one minute to solve it and the authors own solver does not run into a timeout for the instance. 

In total, $28$ authors followed our calls for participation and benchmark instances, thus contributing to a set of $1260$ previously unseen benchmark instances with a variety of $27$ different instance families. 
By filtering out instances with could be solved by Minisat in less than $10$ minutes, we obtained an initial set of $1012$ instances. 

In order to compile a balanced set of $300$ benchmark instances, we used the procedure which is depicted in Algorithm~\ref{algo:select} to select a maximium of $14$ submissions per author. 
Per author, if possible, we first randomly selected $7$ satisfiable and $7$ unsatisfiable instances (lines~3 and~4). 
If this did not yield a total of $14$ instances, we added instances of yet unknown outcome (lines~5-7). 
Of the such obtained $308$ instances, we randomly removed $8$ satisfiable instances, yielding a total of $114$ satisfiable, $78$ unsatisfiable and $108$ instances of unknown result. 

\begin{algorithm}[t]
\DontPrintSemicolon

\KwData{$I$ : Set of Instances, $A$ : Set of Authors}
\KwData{Functions $\alpha : I \rightarrow A$ and $\sigma : I \rightarrow \{\mathsf{sat}, \mathsf{unsat}, \mathsf{unknown}\}$}
\KwResult{$S$ : Set of Selected Instances}
\SetKwFunction{rand}{$\mathsf{random.choice}$}
\BlankLine
$S \leftarrow \emptyset$\;

\For {$a \in A$} {
	$I_a^+ \leftarrow$ \rand{$\{ e \in I \mid \alpha(e) = a \land \sigma(e) = \mathsf{sat} \}$, $k=7$}\;	
	$I_a^- \leftarrow$ \rand{$\{ e \in I \mid \alpha(e) = a \land \sigma(e) = \mathsf{unsat} \}$, $k=7$}\;	
	\If {$|I_a^+|+|I_a^-| < 14$}{
		$l \leftarrow 14 - (|I_a^+|+|I_a^-|)$\;
		$I_a^? \leftarrow$ \rand{$\{ e \in I \mid \alpha(e) = a \land \sigma(e) = \mathsf{unknown} \}$, $k=l$}\;
	}
	$S \leftarrow S \cup I_a^+ \cup I_a^- \cup I_a^?$\;	
}
\Return $S$\;

\caption{Benchmark Instance Selection}
\label{algo:select}
\end{algorithm}

To obtain the final compilation of $400$ benchmark instances, we augmented this set with $100$ instances which have been used in previous competitions. 
We randomly selected $21$ satisfiable, $57$ unsatisfiable and $22$ unknown instances to yield a total of $135$ satisfiable, $135$ unsatisfiable and $130$ instances of unkown result. 
Furthermore, we made sure not to select instances of a family which is already represented in the set of newly submitted instances and excluded random, agile and planning instances (due to the planning track). 

Table~\ref{tab:families} displays the numbers of submitted, interesting and finally selected instances grouped by problem family. 

\begin{table}[h]
\centering
\begin{tabular}{rrr||l}
submitted & interesting & selected & family\\
\hline\hline
6 & 2 & 2 & 01-integer-programming\\
187 & 139 & 14 & antibandwidth\\
40 & 20 & 13 & baseball-lineup\\
393 & 333 & 14 & bitvector\\
20 & 15 & 12 & cellular-automata\\
38 & 18 & 7 & cnf-miter\\
14 & 14 & 14 & coloring\\
20 & 20 & 14 & core-based-generator\\
18 & 13 & 13 & cover\\
106 & 93 & 34 & cryptography\\
20 & 19 & 7 & discrete-logarithm\\
58 & 57 & 7 & edge-matching\\
8 & 5 & 5 & fermat\\
40 & 3 & 0 & flood-it-puzzle\\
20 & 19 & 13 & hgen\\
56 & 48 & 14 & hypertree-decomposition\\
20 & 16 & 14 & influence-maximization\\
20 & 20 & 9 & lam-discrete-geometry\\
20 & 20 & 8 & polynomial-multiplication\\
4 & 2 & 2 & schur-coloring\\
20 & 20 & 12 & station-repacking\\
23 & 19 & 7 & stedman-triples\\
5 & 2 & 2 & sum-subset\\
20 & 20 & 14 & tensors\\
12 & 7 & 7 & termination\\
20 & 16 & 14 & timetable\\
16 & 16 & 14 & tournament\\
36 & 36 & 14 & vlsat\\
\hline
1260 & 1012 & 300 & $\Sigma$
\end{tabular}
\caption{Families and amounts of newly submitted instances}
\label{tab:families}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{c||ccc|c}
 & SAT & UNSAT & UNKNOWN & $\Sigma$\\
\hline\hline
new instances & 114 & 78 & 108 & 300 \\
old instances & 21 & 57 & 22 & 100\\
\hline
$\Sigma$ & 135 & 135 & 130 & 400
\end{tabular}
\caption{Amount of old and new instances by result}
\label{tab:final}
\end{table}


\subsection{Planning Instances}
Classical planning is the problem of finding a sequence of actions -- a plan --
that transforms the world from some initial state to a goal state. In 1992 Kautz
\cite{Kautz1992} proposed to encode planning as satisfiability. In their
encoding the problem of finding a plan of length $i$ (\textit{i.e.,} the
\emph{makespan}) is translated into a Boolean formula $F_i$ that is satisfiable
exactly if a plan of length $i$ \emph{or less} exists. In later encodings
multiple actions can be executed \emph{in parallel} allowing longer plans to be
found by solving smaller formulas \cite{Rintanen2006, Rintanen2007, Balyo2013}.

Finding the minimal makespan $i$ for which $F_i$ is satisfiable is important for
SAT-based planning in general and the generation of this benchmark set in
particular. The minimal makespan depends on the planning task and the used
encoding. The hardest formulas that a SAT-based planner has to solve are usually
the last unsatisfiable $F_i$ before the next higher makespan becomes satisfiable
\cite{Rintanen2006}. Therefore we preferably pick the last unsatisfiable
makespan for each planning task to generate unsatisfiable instances. For
planning tasks where this makespan cannot be determined with available
computational resources, we use a \emph{sequential} encoding, where the minimal
makespan equals the length of the shortest valid plan. Together with known
bounds\footnote{Bounds on plan length are available for some planning tasks from
  the \emph{optimal track} that have \emph{unit cost} actions.} on the optimal
plan length we can generate SAT formulas with predetermined satisfiability for
hard planning problems.

The encodings are generated by the two SAT-based planners \emph{Madagascar}
\cite{Madagascar14} and \emph{Pasar} \cite{Pasar19}. We use Madagascar both in
its default configuration to generate a parallel encoding based on
$\exists$-step plans and to generate the sequential encoding where needed. Pasar
uses the \emph{grounding routine} deployed by the well known planner \emph{Fast
  Downward} \cite{FastDownward06} to translate planning tasks into a different
formalism and then encodes it to SAT using a parallel encoding.

The classical planning benchmarks are selected from the \emph{satisficing} and
\emph{optimal} tracks of the \emph{International Planning Competitions} 2014
\footnote{\url{https://helios.hud.ac.uk/scommv/IPC-14/repository/benchmarksV1.1.zip}}
and 2018 \footnote{\url{https://bitbucket.org/ipc2018-classical/domains}}.

In addition to the classical planning problems, we also include SAT formulas
generated by \emph{Tree-REX} \cite{TreeRex19}; a planner for \emph{Hierarchical
  Task-networks}. In HTN planning the planner is provided with additional domain
knowledge besides the problem description. The HTN benchmarks are provided by
the author of \emph{Tree-REX}.

Table \ref{tab:planningBenchmarkDist} shows the number of benchmarks generated
by each encoding.

\begin{table}[h]
  \caption{Number of benchmarks generated by each encoding.}
  \centering
  \begin{tabular}{@{}|l@{\hspace{3pt}}|l|r|r|@{}}
    % &Encoding& SAT & UNSAT\\
    \hline
    \multicolumn{2}{|@{}l|}{Encoding} & SAT & UNSAT\\
    % \midrule
    \hline
    \textbf{H}  & Tree-REX & 15 & 11\\
    \textbf{P}  & PASAR & 14 & 14\\
    \textbf{ME} & Madagascar $\exists$-step & 5 & 10\\
    \textbf{MS} & Madagascar sequential & 66 & 65\\
    % \midrule
    \hline
    && 100 & 100\\
    % \bottomrule
    \hline
  \end{tabular}
  \label{tab:planningBenchmarkDist}
\end{table}

The benchmarks of the planning track adhere to the naming convention below.
For
% more details and
a complete list of the encoded planning tasks we refer to the
generation script {\color{red}BROKEN LINK}.

${\langle \texttt{SAT/UNSAT} \rangle\_\langle \texttt{encoding} \rangle\_\langle
  \texttt{pathToInstance} \rangle\_\langle \texttt{makespan}
  \rangle\text{.cnf}}$

\subsection{Incremental Library Applications and Instances}

With the introduction of IPASIR (Re-entrant Incremental Solver API), the incremental library track took place for the first time in SAT~Race~2015~\cite{Balyo:2015:SATRace}, and then again in SAT~Competitions 2016 and 2017. 
In SAT~Competitions 2020, we run the track again and selected a total $300$ benchmark instances for the six applications described below. 

\paragraph{Backbone Detection}

The application $\mathsf{genipabones}$ reads a formula from a given file and transforms it using the dual rail encoding, i.e., it replaces each $x$ by $p_x$ and each $\overline{x}$ by $n_x$ and adds clauses of the form ($\overline{p_x} \vee \overline{n_x}$). 
Incrementally, each variable is then checked if it is a backbone variable or not. 
For this application, we selected $50$ of the smallest and easiest satisfiable problems of previous SAT competitions. 

\paragraph{Essential Variables Calculation}

%Given a satisfiable formula $F$, a variable $x$ is essential for the satisfiability of $F$ if a truth value must be assigned to $x$ in each satisfying partial assignment of F. 
The application $\mathsf{genipaessentials}$ incrementally finds all the variables essential for the satisfiability of a given formula by testing each variable using the dual-rail encoded formula. 
For this application, we used the same easy satisfiable problems as in the Backbone Detection application. 

\paragraph{Longest Simple Path Computation}

The application $\mathsf{genipalsp}$ finds the longest simple path in a graph. 
For this application, we selected $50$ instances of the smallest graphs provided by Balyo et al.~\cite{Balyo:2019:LSP}. 

\paragraph{MaxSAT}

The application $\mathsf{genipamax}$ is a trivial partial MaxSAT solver based on adding activation literals to soft clauses and 
subsequent incremental optimization using a cardinality constraint~\cite{Philipp:2015:PBLib} and assumptions. 
For this application, we selected $50$ instances from MaxSAT Evaluation 2019\footnote{\url{https://maxsat-evaluations.github.io/2019/}}. 

\paragraph{QBF}
$\mathsf{Ijtihad}$ is a solver for Quantified Boolean Formulas (QBFs). 
The solver tackles the a formula iteratively, using counterexample-guided expansion~\cite{Bloem:2018:QBFSAT}.
For this application, we selected $50$ instances from QBF Evaluation 2019\footnote{\url{http://www.qbflib.org/qbfeval19.php}}. 

\paragraph{Planning}
$\mathsf{Pasar}$ is a planer which is based on the principles of counter-example guided abstraction refinement(CEGAR)~\cite{Froleyks:2019:Pasar}. 
For this application, we selected $50$ sas planning instances. 


\section{Competition Results}
\label{sec:results}

\todo{New Codebases}

\todo{Winners and Runtime Plots for all the tracks}

\todo{Similarity of Solvers: Rank Correlation}


\section{History of Code and Winning Strategies}
\label{sec:analysis}

In the following we provide an overview on the participating teams and solvers,  and summarize new strategies which are implemented in the award winning solvers of this competition. 

We start with a few remarks on the evolution of code-bases of well known SAT solvers in Section~\ref{sec:codebases}. 
Our survey of the winning strategies in this competition starts with sequential (including incremental) SAT solving in Section~\ref{sec:part:seq}, then we continue with parallel SAT solving in Section~\ref{sec:part:par} and conclude with solvers in the cloud track in Section~\ref{sec:part:cloud}. 


\subsection{On the Shoulders of Giants: The Evolution of SAT Solvers}
\label{sec:codebases}

With a few exceptions, progress in SAT solvers emerged as extensions or forks from existing code bases. 
One well-known tree of development is rooted in the Codebase of the Minisat solver by Niklas Ãen and Niklas SÃ¶renson~\cite{}. 
The most well-known fork of Minisat is the Glucose solver by Gilles Audemard and Laurent Simon~\cite{}.  
The authors of Glucose introduced numerous implementations of successful new methods, e.g., Literal Block Distance (LBD)~\cite{}, which they recently summarized in~\cite{}. 

The SAT solver RISS by Norbert Manthey is a successful and award winning fork of Glucose (version 2.2) in combination with the state-of-the-art preprocessor Coprocessor~\cite{}. 


Another successful string of development is rooted in the CoMinisatPS fork of Minisat by Chanseok Oh~\cite{}. 
Maple first emerged as a series of forks submitted to SAT Competition 2016, introducing the two new branching heuristics Learning Rate based Branching (LRB) and Conflict History based Branching (CHB)~\cite{}. 
The then award winning variant MapleCOMSPS is an implementation of the LRB heuristic on top of CoMinisatPS~\cite{}. 
The award winning derivatives of MapleCOMSPS in SAT Competition 2017 include a new Learnt Clause Minimization (LCM) procedure and the new branching heuristic Distance (Dist). 
This was extended with case-based Chronological Backtracking (ChronoBT) by Ryvchin and Nadel~\cite{}, which resulted in the award winning solver Maple\_LCM\_Dist\_ChronoBT of SAT Competition 2018~\cite{}. 
Its extension MapleLCMDistChronoBT-DL with a new clause learning/forgetting heuristic Duplicate Learnts (DL)~\cite{} won SAT Race 2019. 
The success story of Maple resulted also in numerous submissions of modified Maple solvers in this competition, some of which have been successful (see Section~\ref{}). 

CryptoMinisat by Mate Soos emerged as a fork of Minisat with an extension that brings special handling of xor clauses via gaussian elimination, which is especially effective when solving cryptographic instances~\cite{}. 
Today, CryptoMinisat is more of an independent code base than a Minisat fork coming with award winning individual implementations of recent SAT technology (see Section~\ref{}). 

The SAT solvers of Armin Biere arguably belong to the best and most well-known independent code-bases. 
Lingeling has been around for years now and still we had competitive submissions of its parallel versions Plingeling in this competition. 
CaDiCal came up in SAT Competition 2017 and has been award winning since then. 
The reimplementation of CaDiCal in the new SAT solver Kissat set a new standard in sequential SAT solving in this competition (see Section~\ref{}). 


\subsection{Sequential SAT Solving: Winners of the Main and Incremental Tracks}
\label{sec:part:seq}

A total of $18$ teams submitted their solvers to the main track of the competition.  
Table~\ref{tab:solvers-main} displays an overview of the participating teams, base solvers and their variants. 
$14$ teams submitted variants of a previously existing base solver such as Maple~\cite{} or Cadical~\cite{}. 

In the incremental track, four teams submitted their solvers. 
All solvers in the incremental library track are created from independent code bases. 
Table~\ref{tab:solvers-incremental} displays an overview of the participating teams and solvers. 

\begin{table}[ht!]
\smaller
\arrayrulecolor{gray}
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\bf Team & \bf Base Solver & \bf Variant & \bf Award \\
\hline

\multirow{4}{*}{\stack{Biere, Fazekas, }{Fleury, Heisinger}} & \multirow{3}{*}{Kissat-sc2020} &  -- & \firsto\firstu\\
 &  &  sat & \firsto\firstu\seconds\\
 &  &  unsat & \firstu\thirdp\\
\cline{2-4}
 &  Cadical-sc2020 &  -- & \\
\hline
 
\multirow{3}{*}{Zhang, Cai}
    & \multirow{3}{*}{MapleLCMDistCBT-DL} & Relaxed & \\
 &  & Rel. noTimeParam & \\
 &  & Rel. newTech & \secondo\firsts\\
\hline

\multirow{2}{*}{\stack{Soos, Cai, Devriendt, }{Gocht, Shaw, Meel}}~
 &  \multirow{2}{*}{CryptoMiniSat-CCAnr} &  -- & \thirdo\thirds\secondp\\
 &  &  lsids & \thirdo\thirds\secondp\\
\hline

\stack{Soos, Selman, Kautz, }{Devriendt, Gocht}~ & CryptoMiniSat-WalkSAT & -- & \\
\hline

\multirow{4}{*}{\stack{Hickey, Feng, }{Bacchus}}
 &  &  trail & \secondu\\
 &  &  alluip & \secondu\firstp\\
 & \multirow{-3}{*}{Cadical} &  alluip-trail & \secondu\firstp\\
 \cline{2-4}
 & MapleLCMDist & alluip-trail & \\
\hline

\multirow{3}{*}{Kochemazov} & \multirow{2}{*}{MapleLCMDistCBT} & f2trc & \thirdu\\
 & & f2trc-s & \thirdu\\
 \cline{2-4}
 & MapleLCMDistCBT-DL & f2trc & \thirdu\\
\hline

\stack{Kochemazov, Zaikin, }{Kondratiev, Semenov} ~& MapleLCMDistCBT-DL-v3 & -- & \\
\hline

\multirow{4}{*}{\stack{Lonlac, }{Nguifo}}
 & \multirow{4}{*}{MapleLCMDistCBT-DL-v3} & Undominated & \\
 &  & Undom. Top16 & \\
 &  & Undom. Top24 & \\
 &  & Undom. Top36 & \\
\hline

\multirow{4}{*}{\stack{Tchinda, }{Djamegni}}
 & \multirow{4}{*}{ExMapleLCMDistCBT} & padc\_dl & \\
 &  & padc\_dl\_ovau\_lin & \\
 &  & padc\_dl\_ovau\_exp & \\
 &  & psids\_dl & \\
% \cline{2-3}& Glucose 3.0 & upGlucose-3.0\_PADC\\
\hline

Shaw, Meel & MapleLCMDistCBT-DL-v3 & DurianSat & \\
\hline

\multirow{2}{*}{Chen}
 & \multirow{2}{*}{MapleLCMDistCBT-DL} & Maple\_Mix & \\
 &  & Maple\_Simp & \\
\hline

Riveros & MapleLCMDistCBT & SLIME & \\
\hline

\multirow{3}{*}{Li, Wu, Xu, Chen}
 & \multirow{3}{*}{MapleLCMDistCBT-DL} & Scavel & \\
 &  & Scavel01 & \\
 &  & Scavel02 & \\
\hline

\multirow{2}{*}{\stack{Liang, Oh, Nejati, }{Poupart, Ganesh}}
 & \multirow{2}{*}{MapleCoMsPS\_LRB\_VSIDS\_2} & -- & \\
 &  & init & \\
\hline

\multirow{4}{*}{\stack{Chowdhury, }{MÃ¼ller, You}}
 & \multirow{4}{*}{MapleLCMDistCBT-DL-v2.2} & exp-V-LGB & \\
 &  & exp-V-L & \\
 &  & exp-L & \\
 &  & exp-V & \\
\hline

\multirow{4}{*}{\stack{Li, Luo, Xiao, }{Li, ManyÃ , LÃ¼}}
 & \multirow{4}{*}{MapleCM} & \texttt{+}dist & \\
 &  & \texttt{+}dist\texttt{+}sattime2s\texttt{+}\texttt{-} & \\
 &  & \texttt{+}dist\texttt{+}simp2\texttt{-}\texttt{-} & \\
 &  & used\texttt{+}dist & \\
\hline

Kaiser, Hartung & MapleLCMDist & PauSat & \\
\hline

\multirow{2}{*}{Osama, Wijs} 
 & \multirow{2}{*}{ParaFROST} & -- & \\
 &  & CBT & \\
\hline
\end{tabular}
\caption{Teams and Solvers participating in the Main Tracks (Variant refers to a named configuration or a name of a hack). 
Note that many teams are only responsible for the variant of a previously existing base solver.
Awards were given in Main (m), SAT (s), UNSAT (u), and Planning (p) sub-tracks.}
\label{tab:solvers-main}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[ht]
\smaller
\arrayrulecolor{gray}
\centering
\begin{tabular}{|l|l|c|}
\hline
\bf Team & \bf Solver & \bf Score\\
\hline
\stack{Soos, Cai, Devriendt, }{Gocht, Shaw, Meel}~ & Cryptominisat-5 & 3 \\
\hline
\stack{Biere, Fazekas, }{Fleury, Heisinger}~ & Cadical-sc2020 & 1 \\
\hline
Chen & abcdsat-i20 & 1 \\
\hline
Manthey & Riss-7.1.2 & 1 \\
\hline
\end{tabular}
\caption{Teams and Solvers participating in the Incremental Library Track. Highest Score (=number of won categories) won the track.}
\label{tab:solvers-incremental}
\end{table}

\subsubsection{Kissat} 

If it came to determine an overall winner of all the subtracks in the Main Track of this competition, then it would clearly be Kissat. 
Kissat was submitted in three configurations, including one default configuration and specialized configurations specifically tailored towards satisfiable or unsatisfiable instances, respectively.
Kissat won four awards, by achieving first place in the overall Main Track, first place in its Unsat Sub-Track, second place in the Sat Sub-Track and third place in the Planning Sub-Track. 

Kissat is a reimplementation of Cadical~\cite{Biere:SC2019} in C, thus, taking advantage of finegrained control on the memory layout of new sophisticated data-structures used to manage watchers and clauses, e.g., through binary clause inlining, sentinel values and bit stuffing~\cite{Biere:SC2020}. 

In Kissat, forward subsumption for learned clauses was entirely replaced by more powerful vivification algorithms~\cite{ChuMinLi:2020:Vivification}. 
For irredundant clauses, forward subsumption is still executed together with variable eliminiation during pre- and inprocessing, but only in a very focused way, by carefully monitoring variable occurences~\cite{Biere:SC2020}.

Kissat also comes with a refined scheduling of inprocessing procedures based on growing conflict intervals. 
Regarding the duration of alternating restart modes, the Kissat authors considered conflict-rate being too unstable, and come up with ``ticks'', which is a new method to measure the length of two alternating restart-modes based on the approximate number of cache-line accesses. 

Considering target phases~\cite{Biere:SC2019}, Kissat authors come up with an innovative use-case for autarkies to account for saved phases. 
Before each rephasing step they compute the largest autarky of the full assignment with an algorithm described in~\cite{Kiesl:2019:Autarkies}. 
The authors claim this autarky contains satisfying assignments of disconnected components which otherwise would get lost, and thus next, they are considered in variable elimination. 

\subsubsection{CryptoMiniSat-CCAnr and CryptoMiniSat5}

This solver won four awards, two third places in the Main Track and its Sat Subtrack, one second place in the Planning Subtrack, and the first place in the Incremental Library Track. 

To the Main track, the solver was submitted in two variants, with and without LSIDS~\cite{Shaw:2020:LSIDS},
which is a phase selection heuristic similar to VSIDS and, if active, it is used whenever backtracking is chronological~\cite{Nadel:2018:CBT}. On top of that, CryptoMiniSat is periodically switching between polarity saving and stable phases~\cite{Biere:SC2019}.

By tight integration of CCAnr~\cite{Cai:2015:CCAnr}, a stochastic local search (SLS) solver, into CryptoMinisat, CryptoMiniSat-CCAnr combines the two SAT solving strategies, SLS and CDCL, in a unique way. 
The solver runs local search for a short period of time in regular intervals and (just in case SLS finds no solution)
changes the polarities according to the best assignment found by the SLS solver, which is known as ``rephasing'' from CaDiCal~\cite{Biere:SC2019}.
Moreover, it bumps VSIDS scores of the first 100 variables contained in the clauses which the SLS solver considers most hard to satisfy~\cite{Soos:SC2020}.

SLS is tightly integrated and thus is aware of changes occurring during inprocessing. 
Inprocessing now also includes ternary resolution and vivification is used a lot more~\cite{ChuMinLi:2020:Vivification}. 
All modifictions in CrytoMiniSat are fully integrated with resprect to incremental use-cases including the use of assumptions. 

CryptoMiniSat periodically changes decay factors of its branching heuristics, thus avoiding usage of a ``single best'' configuration.

This version of CryptoMiniSat comes with an highly optimized implementation of Gauss-Jordan Elimination~\cite{Soos:2020:CNFXOR}. 
CryptoMiniSat includes BreakId, a sub-system that calculates symmetry breaking clauses, which it calls on every 5th inprocessing iteration~\cite{Devriendt:2016:BreakId}.

\subsubsection{Cadical Hacks: Trail and AllUip}

This modified version of last years CaDiCal was submitted with the two new methods \emph{Trail Saving}~\cite{Hickey:2020:TrailSaving} and \emph{Stable AllUIP}~\cite{Bacchus:SC2020}. 
It was submitted in three configurations, one with both methods activated and two other with each having only one of them active. 
The \emph{Stable AllUIP} variants seemed most successful, in most tracks they were the hightest ranked variants and won the first price in the Planning Sub-Track. 
However, in the Unsat Sub-Track, the \emph{Trail Saving} variant performed best.

With \emph{Trail Saving}, the solver caches backtracked slices of the assignment trail in order to reuse them when the same decisions are repeated. 
The authors came up with sophisticated control mechanisms on when and how to reuse the cached trail slices and when to flush that cache. 

With \emph{Stable AllUip}, the solver continues resolution beyond the 1st Unit Implication Point (1-UIP) during clause-learning~\cite{Zhang:2001:ClauseLearning} as long that does not increase the LBD value of the learnt clause and then keeps that clause only if it has a smaller size than then 1-UIP clause. 
The authors came up with an additional heuristic to limit the amount of attempted AllUip learning attempts, if the fraction of unsuccessful alluip learning attempts gets to high, which they check and adapt on every restart~\cite{Bacchus:SC2020}. 


\subsubsection{Relaxed MapleLCMDistCBT-DL newTech}

The variant \emph{newTech} of the \emph{Relaxed} fork of the MapleLCMDistCBT-DL was especially successful on satisfiable instances. 
The solver won two awards, second price in Main Track and first price in its Sat Sub-Track.

The \emph{Relaxed} fork of MapleLCMDistCBT-DL already participated in SAT Race 2019~\cite{Xindi:SC2019}.
They integrated the local search solver CCAnr by extending the current partial assignment of the CDCL solver to a full assignment and for a fixed interval start the local search solver to try to find a nearby solution. 

In the new versions of the Relaxed family of Maple solvers~\cite{Xindi:SC2020}, they probabilistically switch between several modes of phase selection, and some of these modes reuse the maximal partially satisfying assignments as determined by the local search solver.

For their successful \emph{newTech} variant~\cite{Xindi:SC2020} of their fork, they count the number of variable-occurrences in unsat clauses for each flip in the local search solver. 
Then they reuse this information to priorize variables in a modified branching heuristic. 


\subsubsection{MapleLCMDistCBT F2TRC}

The MapleLCMDistCBT F2TRC forks were ranked third in the Unsat Sub-Track of the Main-Track.

The F2TRC forks aim at introducing determinism in winning solvers of previous competition, e.g., by replacing time-based intervals for LRB-VSIDS switches by a heuristic based on the conflict-count~\cite{Kochemazov:SC2020}. 

Another modification aims at improving three-tier clause management based on the three tiers core, tier2 and local as it was introduced by Chanseok Oh~\cite{Oh:2015:satunsat}.
In order to limit growth of the core-tier, they introduce a limit on the number of core clauses, and whenever that is reached some of the low-ranked inactive core clauses are moved from core to tier2 and the limit is increased. 
In order to prevent tier2 from starving, they replaced the conflict limit for tier2 reduction by a size limit, then only half of the most recently inactive clauses is moved to local tier~\cite{Kochemazov:SC2020}. 


\subsection{Parallel SAT Solving: Winners of the Parallel Track}
\label{sec:part:par}

\begin{table}[ht]
\smaller
\centering
\begin{tabular}{|l|l|l|}
\hline
\bf Team & \bf Solver & \bf Awards\\
\hline
\multirow{2}{*}{\stack{Vallade, Le Frioux, Baarir, }{Sopena, Kordon}}~
 & P-MCOMSPS-STR-32 & \firsto \firsts \secondu \\
 & P-MCOMSPS-STR-64 & \firsto \\
\hline
\multirow{2}{*}{\stack{Biere, Fazekas, }{Fleury, Heisinger}}
 & Plingeling & \secondo \thirds \firstu \\
 & Treengeling & \\
\hline
\multirow{2}{*}{Nabeshima, Inoue}
 & ManyGlucose-32 & \thirdo \thirdu \\
 & ManyGlucose-64 & \thirdu \\
\hline
\multirow{2}{*}{Tchinda, Djamegni}
 & painlessmaplevone & \seconds \\
 & painlessmaplevtwo & \seconds \\
\hline
Li, Wu, Xu, Chen & syrupscavel & \\
\hline
Chen & abcdsatptwenty & \\
\hline
\end{tabular}
\caption{Teams and Solvers participating in the parallel track.}
\end{table}

\subsubsection{P-MCOMSPS-STR} 

P-MCOMSPS-STR is built from the Painless framework~\cite{Frioux:2017:Painless} for solver parallization and uses the MapleCOMSPS solver~\cite{Liang:2017:Maplecomsps} as a building block. 
The authors submitted a 32 and 64 core version and won 3 awards, first place in the overall ranking, first place in the SAT ranking and second in the UNSAT ranking. 
The 64 core version performed slightly worse than the 32 core version in the overall ranking, and that gap is bigger for the SAT-only ranking and worst for the UNSAT-only ranking, such that in the separate rankings only the 32 core version is responsible for winning the prices. 

The painless framework uses a generic interface to integrate a solver and abstracts away the implementation details of parallelism and concurrenct data-structures, such that implementations within the framework boil down to implementing parallelization and clause sharing strategies. 

In the submitted version of P-MCOMSPS-STR, diversification is done via diverse fixed configurations of branching strategies (LRB and VSIDS) and sparse random intialization of variable polarities like in~\cite{Balyo:2015:Hordesat}. 
One instance of their ``sequential engines'' performs concurrent clause strengtheing as described in~\cite{Wieringa:2013:CCS} and another single instance is configured to perform Gaussian eliminiation during preprocessing. 

For sharing they use an all-to-all strategy with a regular exchange of a fixed size buffer and a dynamic clause lbd limit. 


\subsubsection{Plingeling} 

Plingeling got three awards, first place in the Unsat Category, third place in Sat and second place in the overall evaluation. 
Plingeling is an extension of the well-known SAT solver Lingeling~\cite{} and did not change since 2016. 
Via a global master queue, Plingeling shares unit clauses, equivalences and short clauses with clauses with at most 40 literals and glucose LBD level of at most 8 and random seeds, e.g., for diversification via default polarities.~\cite{} 


\subsubsection{ManyGlucose}

The authors of ManyGlucose received two awards in this competition. 
ManyGlucose was submitted in 32-core and 64-core variants~\cite{}.
The 32-core variant won two third prices, one in the overall evaluation and one in the unsat sub-track. 
ManyGlucose is a fork of GlucoseSyrup~\cite{} and uses strategies similar to those known from ManySAT~\cite{} in order to achive deterministic solver runs. 


\subsubsection{Painless Maple} 

The authors Painless Maple received one award for second best performance in the Sat sub-track of the parallel track. 
However, the solver was last in the Unsat sub-track. 

The authors implemented a couple of modifications (\todo{name them}) on top of the MapleLCMDistChronoBT~\cite{} solver, which they also submitted in various configurations to the main track. 
The two submissions in the parallel track use painless as a parallelization framework. 

Their sharing strategy includes splitting their workers into two categories, where the solvers in the first category only export clauses and only solvers in the second category import and export clauses. 

They submitted two variants with different diversification strategies. 
In their first variant they use different configurations of configurations of the newly implemented strategies. 
The second variant uses a more classic diverisification strategy by randomizing polarities and variable scores. 


\subsection{Massively Parallel SAT Solving: Winners of the Cloud Track}
\label{sec:part:cloud}

\begin{table}[ht]
\smaller
\centering
\begin{tabular}{|l|l|l|}
\hline
\bf Team & \bf Solver & \bf Rank\\
\hline
Schreiber & mallob-mono & 1\\
\hline
\stack{Ehlers, Kulczynski, }{Nowotka, Sieweck}~ & TopoSAT2 & 2\\
\hline
Riveros & Slime & 3\\
\hline
\multirow{2}{*}{\stack{Biere, Fazekas, }{Fleury, Heisinger}}~ & Paracooba & -\\
& Paracooba-March & -\\
\hline
Hartung & CTSat & -\\
\hline
\end{tabular}
\caption{Teams and Solvers participating in the cloud track.}
\end{table}


\subsubsection{Mallob Mono}

The massively parallel SAT solver Mallob is based on HordeSAT~\cite{}. 
Mallob is a framework for massivley parallel and distributed \emph{malleable job scheduling}, and thus can perform dynamic load balancing on many jobs of varying priorty.
However, in the submitted version Mallob Mono, this functionality is disabled, as traditionally in SAT Competition all resources are busy solving one single instance only. 

Mallob uses the the 2018 version \emph{bcj} of Lingeling~\cite{} and employs diversification via randomized sparse initialization of branching scores just like it is done in Plingeling~\cite{}. 
Every 14th process, however, runs the local search solver YalSAT~\cite{}. 

Clause exchange is handled considerably different from previous approach. 
Workers are organized in a binary tree and clauses are asynchronously aggregated in a buffer which is passed along this tree from leafs to the root. 
Each node performs a three way merge of the local and the two incoming buffers. 
Only the final aggragate at the root of that binary tree is broadcast to all the solvers. 

To limit sharing, there is a global size limit of $5$ for all the shared clauses. 
The buffer has a limited size as well, but that size depends on the position in the binary tree, with root having the largest buffer and leaves the smallest. 
Clauses are sorted by there size during aggregation, such that smaller clauses are preferred over longer clauses. 
Duplicates are filtered out using a bloom filter, which is cleared periodically. 
There is a special filter for duplicate unit-clause which especially useful during startup. 


\subsubsection{TopoSAT 2}

TopoSAT~2 is a massively parallel SAT solver using Glucose~3 with lockfree clause-exchange as in ManySAT~\cite{} on each node, and using MPI to share clauses between nodes. 

In Toposat~2, diversification is done via different strategies used for branching, restarting and clause forgetting. 
Clauses are strengthened before they are exported. 
Locally, clause import is delayed until the local trail-size is low in a monitored period. 


\subsubsection{Slime}

Slime is build on top of MapleLCMDistChronoBT and was first submitted to SAT Race 2019 suggesting the new phase selection heuristic~\cite{}. 
The 2020 version apparently adds some kind of periodic randomization in geometrically increasing intervals~\cite{}. 
Even thought the sequential solver was unsuccessful in the Main Track, its MPI-based cloud version, won the third price in this competitions cloud track. 


\section{Conclusion and Prospects}
\label{sec:conclusion}

\todo{Summarize Winning Strategies}

\todo{Ideas for Future Competitions}

\section*{Acknowledgements}
Martin Suda was supported by the ERC Consolidator grant AI4REASON no. 649043 under the EU-H2020 programme,
the Czech Science Foundataion project 20-06390Y, and the project RICAIP, no. 857306 under the EU-H2020 programme.

\bibliographystyle{elsarticle-num}
\bibliography{main}

\end{document}
